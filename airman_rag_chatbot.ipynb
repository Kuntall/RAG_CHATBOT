{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# AIRMAN — Technical Assignment Solution\n\nThis notebook is organized **exactly** in this order:\n\n1. **Level 1 (Compulsory)** — PDF ingestion, chunking with LangChain text splitter, vector index, strict grounded answering with citations, minimal API.\n2. **Level 2 (Optional — Option 1)** — Hybrid retrieval (**BM25 + Vector**) + **Cross-Encoder reranker**.\n3. **Question set + Evaluation** — 50 questions + baseline (Level 1) vs hybrid (Level 2) comparison.\n\nThe system follows a strict grounding rule:\n\n> **This information is not available in the provided document(s).**\n\nwhenever the answer cannot be supported from retrieved text.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Install dependencies (run once in a fresh environment)\n%pip -q install \"langchain>=0.2.0\" \"langchain-community>=0.2.0\" \"langchain-text-splitters>=0.2.0\" \\\n    \"pypdf>=4.0.0\" \"faiss-cpu>=1.7.4\" \"sentence-transformers>=2.2.2\" \\\n    \"rank-bm25>=0.2.2\" \"fastapi>=0.110.0\" \"uvicorn>=0.27.0\" \"pydantic>=2.6.0\""
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport re\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport faiss\n\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom rank_bm25 import BM25Okapi\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "DATA_DIR = Path(\"./data\")\nINDEX_DIR = Path(\"./index_store\")\n\nEMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nRERANK_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 150\n\nTOP_K_VECTOR = 12\nTOP_K_BM25 = 24\nTOP_K_FINAL = 6\n\nRERANK_MIN_SCORE = 4.0\n\nREFUSAL_TEXT = \"This information is not available in the provided document(s).\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Level 1 (Compulsory)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Level 1 Overview (Compulsory)\n\nWhat we implement and why:\n\n- **PDF loading**: we read your provided aviation PDFs as the knowledge source.\n- **Chunking (LangChain RecursiveCharacterTextSplitter)**: splits long pages into overlapping chunks so retrieval can match small sections precisely.\n- **Embeddings + Vector index (FAISS)**: creates a fast semantic search index over chunks.\n- **Grounded answering + citations**: returns an answer derived only from retrieved chunks and shows where it came from (source PDF + page).\n- **Strict refusal**: if retrieval confidence is too low or context does not support the query, we refuse with the exact required sentence.\n- **Minimal API (FastAPI)**: basic endpoints for health, ingest, and ask.\n\nDesign choice: we keep generation lightweight and grounded. The notebook focuses on retrieval + safe answer composition rather than free-form LLM generation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Load PDFs\n\nWe load PDFs using `PyPDFLoader` so we get text per page with metadata (source, page).\nThis metadata is later used for citations.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def list_pdf_files(data_dir: Path) -> List[Path]:\n    return sorted([p for p in data_dir.glob(\"*.pdf\") if p.is_file()])\n\n\ndef load_pdfs(pdfs: List[Path]) -> List[Dict[str, Any]]:\n    docs = []\n    for pdf in pdfs:\n        loader = PyPDFLoader(str(pdf))\n        pages = loader.load()\n        for p in pages:\n            md = dict(p.metadata or {})\n            md[\"source\"] = pdf.name\n            docs.append({\"text\": p.page_content, \"metadata\": md})\n    return docs"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Chunking (LangChain text splitter)\n\nWe use **LangChain `RecursiveCharacterTextSplitter`** because PDF pages can be long.\nChunking improves retrieval accuracy, and overlap prevents losing context across boundaries.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_text_splitter() -> RecursiveCharacterTextSplitter:\n    return RecursiveCharacterTextSplitter(\n        chunk_size=CHUNK_SIZE,\n        chunk_overlap=CHUNK_OVERLAP,\n        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    )\n\n\ndef normalize_whitespace(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n\n\ndef chunk_documents(docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    splitter = build_text_splitter()\n    chunks = []\n    for d in docs:\n        text = (d[\"text\"] or \"\").strip()\n        if not text:\n            continue\n        parts = splitter.split_text(text)\n        for i, part in enumerate(parts):\n            md = dict(d[\"metadata\"])\n            md[\"chunk_in_page\"] = i\n            chunks.append({\"text\": normalize_whitespace(part), \"metadata\": md})\n    return chunks"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Vector index (FAISS)\n\nWe embed each chunk and store it in FAISS for fast semantic search.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass VectorIndex:\n    embed_model_name: str\n    dim: int\n    faiss_index: Any\n    texts: List[str]\n    metadatas: List[Dict[str, Any]]\n\n    @classmethod\n    def build(cls, chunks: List[Dict[str, Any]], embed_model_name: str) -> \"VectorIndex\":\n        model = SentenceTransformer(embed_model_name)\n        texts = [c[\"text\"] for c in chunks]\n        metas = [c[\"metadata\"] for c in chunks]\n\n        emb = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n        emb = np.asarray(emb, dtype=\"float32\")\n\n        dim = emb.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(emb)\n\n        return cls(embed_model_name=embed_model_name, dim=dim, faiss_index=index, texts=texts, metadatas=metas)\n\n    def save(self, index_dir: Path) -> None:\n        index_dir.mkdir(parents=True, exist_ok=True)\n        faiss.write_index(self.faiss_index, str(index_dir / \"faiss.index\"))\n        payload = {\n            \"embed_model_name\": self.embed_model_name,\n            \"dim\": self.dim,\n            \"texts\": self.texts,\n            \"metadatas\": self.metadatas,\n        }\n        (index_dir / \"store.json\").write_text(\n            json.dumps(payload, ensure_ascii=False),\n            encoding=\"utf-8\"\n        )\n\n    @classmethod\n    def load(cls, index_dir: Path) -> \"VectorIndex\":\n        payload = json.loads((index_dir / \"store.json\").read_text(encoding=\"utf-8\"))\n        index = faiss.read_index(str(index_dir / \"faiss.index\"))\n        return cls(\n            embed_model_name=payload[\"embed_model_name\"],\n            dim=payload[\"dim\"],\n            faiss_index=index,\n            texts=payload[\"texts\"],\n            metadatas=payload[\"metadatas\"],\n        )\n\n    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:\n        model = SentenceTransformer(self.embed_model_name)\n        q = model.encode([query], normalize_embeddings=True)\n        q = np.asarray(q, dtype=\"float32\")\n\n        scores, ids = self.faiss_index.search(q, top_k)\n        results = []\n        for score, idx in zip(scores[0].tolist(), ids[0].tolist()):\n            if idx < 0:\n                continue\n            results.append(\n                {\n                    \"id\": int(idx),\n                    \"score\": float(score),\n                    \"text\": self.texts[idx],\n                    \"metadata\": self.metadatas[idx],\n                }\n            )\n        return results"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Grounded answering + citations + refusal\n\nWe answer only using retrieved chunks and cite sources.\nIf nothing supports the question, we return the exact refusal text.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def format_citation(md: Dict[str, Any]) -> str:\n    doc = md.get(\"source\", \"unknown\")\n    page = md.get(\"page\", None)\n    if page is None:\n        return f\"{doc} | chunk\"\n    return f\"{doc} | page {int(page) + 1}\"\n\n\ndef simple_tokenize(text: str) -> List[str]:\n    text = normalize_whitespace(text).lower()\n    return re.findall(r\"[a-z0-9']+\", text)\n\n\ndef answer_from_context(question: str, chunks: List[Dict[str, Any]]) -> str:\n    if not chunks:\n        return REFUSAL_TEXT\n\n    joined = normalize_whitespace(\" \".join([c[\"text\"] for c in chunks]))\n    q = normalize_whitespace(question).lower()\n    keywords = [w for w in simple_tokenize(q) if len(w) >= 4] or simple_tokenize(q)\n\n    sentences = re.split(r\"(?<=[\\.\\?\\!])\\s+\", joined)\n    scored = []\n    for s in sentences:\n        s_l = s.lower()\n        hits = sum(1 for k in keywords if k in s_l)\n        if hits > 0:\n            scored.append((hits, s.strip()))\n    scored.sort(reverse=True)\n\n    if not scored:\n        return REFUSAL_TEXT\n\n    best = [s for _, s in scored[:3]]\n    ans = \" \".join(best).strip()\n    return ans if ans else REFUSAL_TEXT\n\n\ndef ask_level1(question: str, vindex: VectorIndex, top_k: int = 6, debug: bool = False) -> Dict[str, Any]:\n    hits = vindex.search(question, top_k)\n    answer_text = answer_from_context(question, hits)\n\n    if answer_text == REFUSAL_TEXT:\n        return {\"answer\": REFUSAL_TEXT, \"citations\": [], \"chunks\": hits if debug else []}\n\n    citations = [format_citation(h[\"metadata\"]) for h in hits]\n    out = {\"answer\": answer_text, \"citations\": citations}\n\n    if debug:\n        out[\"chunks\"] = [\n            {\n                \"citation\": format_citation(h[\"metadata\"]),\n                \"score\": h[\"score\"],\n                \"text_snippet\": h[\"text\"][:300],\n            }\n            for h in hits\n        ]\n    return out"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Ingestion and persistence\n\nWe ingest PDFs, chunk them, embed chunks, and persist FAISS + metadata for reuse.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def ingest_level1(data_dir: Path, index_dir: Path) -> VectorIndex:\n    pdfs = list_pdf_files(data_dir)\n    if not pdfs:\n        raise FileNotFoundError(f\"No PDFs found in {data_dir.resolve()}\")\n\n    raw_docs = load_pdfs(pdfs)\n    chunks = chunk_documents(raw_docs)\n\n    vindex = VectorIndex.build(chunks, EMBED_MODEL_NAME)\n    vindex.save(index_dir)\n\n    meta = {\n        \"pdfs\": [p.name for p in pdfs],\n        \"chunk_size\": CHUNK_SIZE,\n        \"chunk_overlap\": CHUNK_OVERLAP,\n        \"embed_model\": EMBED_MODEL_NAME,\n        \"num_chunks\": len(vindex.texts),\n    }\n    (index_dir / \"meta_level1.json\").write_text(\n        json.dumps(meta, ensure_ascii=False, indent=2),\n        encoding=\"utf-8\"\n    )\n    return vindex\n\n\ndef load_level1(index_dir: Path) -> VectorIndex:\n    return VectorIndex.load(index_dir)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 1 — Minimal API\n\nFastAPI endpoints to ingest and ask questions.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "app = FastAPI(title=\"AIRMAN RAG (Level 1 + Level 2)\")\n\nclass IngestRequest(BaseModel):\n    data_dir: str = \"./data\"\n    index_dir: str = \"./index_store\"\n\nclass AskRequest(BaseModel):\n    question: str\n    debug: bool = False\n    mode: str = \"level1\"\n\n\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n\n\n@app.post(\"/ingest\")\ndef ingest(req: IngestRequest):\n    vindex = ingest_level1(Path(req.data_dir), Path(req.index_dir))\n    return {\"status\": \"ingested\", \"num_chunks\": len(vindex.texts)}\n\n\n@app.post(\"/ask\")\ndef ask_api(req: AskRequest):\n    if req.mode == \"level1\":\n        vindex = load_level1(INDEX_DIR)\n        return ask_level1(req.question, vindex, top_k=TOP_K_FINAL, debug=req.debug)\n\n    vindex, bm25 = load_level2(INDEX_DIR)\n    return ask_level2(req.question, vindex, bm25, debug=req.debug)\n\n\n# Run:\n# import uvicorn\n# uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Level 2 (Optional) — Option 1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Level 2 (Optional) — Option 1 Overview\n\nOption 1 adds accuracy improvements on top of Level 1:\n\n- **BM25 keyword retrieval**: improves recall for exact terms, acronyms, numbers (e.g., QNH, FL, hPa).\n- **Hybrid candidate pool (BM25 + Vector)**: merges both retriever results to reduce misses.\n- **Cross-Encoder reranker**: re-scores query-chunk pairs with a stronger model so the final top chunks are more precise.\n- **Confidence gating**: if reranker score is below threshold, we refuse to avoid hallucination.\n\nThis is a common production-grade pattern: *retrieve broadly, rerank carefully, answer strictly from context*.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 2 — BM25 retriever\n\nBM25 is lexical retrieval. It is strong for exact strings, acronyms, and numbers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass BM25Index:\n    bm25: Any\n    texts: List[str]\n    metadatas: List[Dict[str, Any]]\n\n    @classmethod\n    def build(cls, texts: List[str], metadatas: List[Dict[str, Any]]) -> \"BM25Index\":\n        tokenized = [simple_tokenize(t) for t in texts]\n        bm25 = BM25Okapi(tokenized)\n        return cls(bm25=bm25, texts=texts, metadatas=metadatas)\n\n    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:\n        q = simple_tokenize(query)\n        scores = self.bm25.get_scores(q)\n        top_idx = np.argsort(scores)[::-1][:top_k]\n        results = []\n        for idx in top_idx.tolist():\n            results.append(\n                {\n                    \"id\": int(idx),\n                    \"score\": float(scores[idx]),\n                    \"text\": self.texts[idx],\n                    \"metadata\": self.metadatas[idx],\n                }\n            )\n        return results"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 2 — Hybrid merge + reranking\n\nWe merge candidates from vector + BM25 and rerank with a cross-encoder for precision.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def merge_candidates(vec_hits: List[Dict[str, Any]], bm25_hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    merged = {}\n    for h in vec_hits:\n        idx = h[\"id\"]\n        merged[idx] = {\n            \"id\": idx,\n            \"text\": h[\"text\"],\n            \"metadata\": h[\"metadata\"],\n            \"vector_score\": float(h[\"score\"]),\n            \"bm25_score\": None,\n            \"retrievers\": [\"vector\"],\n        }\n\n    for h in bm25_hits:\n        idx = h[\"id\"]\n        if idx not in merged:\n            merged[idx] = {\n                \"id\": idx,\n                \"text\": h[\"text\"],\n                \"metadata\": h[\"metadata\"],\n                \"vector_score\": None,\n                \"bm25_score\": float(h[\"score\"]),\n                \"retrievers\": [\"bm25\"],\n            }\n        else:\n            merged[idx][\"bm25_score\"] = float(h[\"score\"])\n            merged[idx][\"retrievers\"] = sorted(list(set(merged[idx][\"retrievers\"] + [\"bm25\"])))\n\n    return list(merged.values())\n\n\ndef rerank(query: str, candidates: List[Dict[str, Any]], model_name: str, top_k: int) -> List[Dict[str, Any]]:\n    if not candidates:\n        return []\n    ce = CrossEncoder(model_name)\n    pairs = [(query, c[\"text\"]) for c in candidates]\n    scores = ce.predict(pairs)\n\n    for c, s in zip(candidates, scores.tolist()):\n        c[\"rerank_score\"] = float(s)\n\n    candidates.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n    return candidates[:top_k]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Level 2 — Ingest/load + ask\n\nLevel 2 reuses the Level 1 vector store and adds BM25 and reranking at query time.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def ingest_level2(data_dir: Path, index_dir: Path) -> Tuple[VectorIndex, BM25Index]:\n    vindex = ingest_level1(data_dir, index_dir)\n    bm25 = BM25Index.build(vindex.texts, vindex.metadatas)\n\n    meta = json.loads((index_dir / \"meta_level1.json\").read_text(encoding=\"utf-8\"))\n    meta[\"bm25_enabled\"] = True\n    meta[\"rerank_model\"] = RERANK_MODEL_NAME\n    (index_dir / \"meta_level2.json\").write_text(\n        json.dumps(meta, ensure_ascii=False, indent=2),\n        encoding=\"utf-8\"\n    )\n\n    return vindex, bm25\n\n\ndef load_level2(index_dir: Path) -> Tuple[VectorIndex, BM25Index]:\n    vindex = VectorIndex.load(index_dir)\n    bm25 = BM25Index.build(vindex.texts, vindex.metadatas)\n    return vindex, bm25\n\n\ndef ask_level2(question: str, vindex: VectorIndex, bm25: BM25Index, debug: bool = False) -> Dict[str, Any]:\n    vec_hits = vindex.search(question, TOP_K_VECTOR)\n    bm_hits = bm25.search(question, TOP_K_BM25)\n\n    candidates = merge_candidates(vec_hits, bm_hits)\n    reranked = rerank(question, candidates, RERANK_MODEL_NAME, TOP_K_FINAL)\n\n    if not reranked:\n        return {\"answer\": REFUSAL_TEXT, \"citations\": [], \"chunks\": reranked if debug else []}\n\n    best_score = reranked[0].get(\"rerank_score\", -1.0)\n    if best_score < RERANK_MIN_SCORE:\n        return {\"answer\": REFUSAL_TEXT, \"citations\": [], \"chunks\": reranked if debug else []}\n\n    answer_text = answer_from_context(question, reranked)\n    if answer_text == REFUSAL_TEXT:\n        return {\"answer\": REFUSAL_TEXT, \"citations\": [], \"chunks\": reranked if debug else []}\n\n    citations = [format_citation(c[\"metadata\"]) for c in reranked]\n    out = {\"answer\": answer_text, \"citations\": citations}\n\n    if debug:\n        out[\"chunks\"] = [\n            {\n                \"citation\": format_citation(c[\"metadata\"]),\n                \"rerank_score\": c.get(\"rerank_score\"),\n                \"retrievers\": c.get(\"retrievers\"),\n                \"vector_score\": c.get(\"vector_score\"),\n                \"bm25_score\": c.get(\"bm25_score\"),\n                \"text_snippet\": c[\"text\"][:300],\n            }\n            for c in reranked\n        ]\n    return out"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Question set + Evaluation (Level 1 + Level 2 comparison)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Question Set + Evaluation\n\nWe include:\n\n- **50 questions** (20 factual, 20 applied, 10 higher-order).\n- **Comparison**:\n  - Level 1 baseline = vector-only retrieval\n  - Level 2 = hybrid retrieval with reranking\n- **Simple metrics**:\n  - refusal rates\n  - answer rates\n  - number of citations produced\n\nThis evaluation is meant to demonstrate retrieval improvements and safe behavior, not to claim perfect correctness without human review.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "QUESTIONS = [\n    \"What is the definition of meteorology?\",\n    \"What is the definition of the atmosphere?\",\n    \"What is the approximate composition of dry air by volume in the troposphere?\",\n    \"In the ISA, what is the sea level standard temperature?\",\n    \"In the ISA, what is the sea level standard pressure in hPa?\",\n    \"In the ISA, what is the standard lapse rate below 11 km?\",\n    \"What does the tropopause mark in terms of temperature change with height?\",\n    \"What are two reasons for studying meteorology for aviation?\",\n    \"What is the role of the Air Data Computer (ADC) in an aircraft?\",\n    \"What are the two ADC system types mentioned and how do they differ at a high level?\",\n    \"In FMC initialization, what is checked or input on the IDENT and POS INIT pages?\",\n    \"What is the purpose of the CDU scratchpad in an FMC?\",\n    \"What does the term 'Decision Point Procedure' relate to in fuel policy?\",\n    \"How is an 'Isolated aerodrome' defined in fuel planning context?\",\n    \"What is the difference between a QDR and a QDM in VOR terminology?\",\n    \"What does RNAV stand for in the context of navigation systems?\",\n    \"In the ISA deviation calculation, how is deviation computed from actual and ISA temperature?\",\n    \"What does the document say about ozone hazards at high altitude?\",\n    \"What does system redundancy mean in the air data system context?\",\n    \"What does the document describe as the main purpose of flight planning?\",\n    \"If an outside air temperature of -30°C is measured at FL200, what is the ISA temperature deviation?\",\n    \"If the tropopause is reported at FL330, what can you infer about significant cloud tops relative to it?\",\n    \"If the ADC on one side fails, what arrangements can allow the captain's instruments to be fed from the other side?\",\n    \"During FMC pre-flight initialization, what sequence of pages would you expect to use after IDENT?\",\n    \"If the navigation database is out of date, what action is described to activate the next cycle?\",\n    \"Given a flight to an isolated aerodrome, what additional fuel requirement is described for turbine aircraft?\",\n    \"In decision point procedure planning, what does contingency fuel between departure and decision point enable?\",\n    \"If asked for the common emergency VHF frequency, how would you answer using only the document text?\",\n    \"If a question is not supported by any retrieved chunk, what exact refusal must your system return?\",\n    \"If BM25 returns many candidates but vector retrieval returns few, how can hybrid retrieval help?\",\n    \"Why might chunk overlap reduce retrieval misses in dense technical text?\",\n    \"How would you cite an answer when you only have chunk metadata for source and page?\",\n    \"How would you decide to refuse answering when retrieval confidence is low?\",\n    \"If a user asks about something outside aviation docs, how should the assistant respond?\",\n    \"How would you debug a wrong answer: what retrieved chunks would you inspect first?\",\n    \"If vector similarity is high but the chunk is off-topic, how does a cross-encoder reranker help?\",\n    \"If you must show top 3 chunks in debug mode, what fields would you include in the response?\",\n    \"If a user asks for a multi-step explanation, how can you keep it grounded in retrieved text?\",\n    \"If you use FAISS with normalized embeddings and inner product, what similarity measure does it approximate?\",\n    \"If pages are 0-indexed by the loader, how do you show human-friendly page numbers in citations?\",\n    \"Compare vector-only retrieval vs hybrid retrieval: why can hybrid improve recall for rare acronyms and numbers?\",\n    \"Describe a failure mode where BM25 helps but reranking is still necessary.\",\n    \"When could a high rerank score still lead to a wrong answer, and how would you mitigate it?\",\n    \"Explain how you would measure retrieval hit-rate without manual labeling, and what are its limitations.\",\n    \"Propose a confidence thresholding approach using reranker scores and how it triggers refusal.\",\n    \"Explain how chunk size affects citations quality and answer completeness in a textbook PDF.\",\n    \"If two chunks disagree, how would you craft an answer that stays faithful and notes the condition?\",\n    \"How would you extend the system to handle follow-up questions while keeping grounding strict?\",\n    \"Explain why showing retrieved chunks in debug mode helps prevent hallucinations during development.\",\n    \"Explain how you would generate and curate a 50-question set to cover factual, applied, and reasoning skills.\",\n]\nlen(QUESTIONS)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def sentence_split(text: str) -> List[str]:\n    text = normalize_whitespace(text)\n    if not text:\n        return []\n    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n    return [p.strip() for p in parts if p.strip()]\n\n\ndef faithfulness_score(answer: str, retrieved_text: str) -> float:\n    ans_sents = sentence_split(answer)\n    if not ans_sents:\n        return 0.0\n    ctx = normalize_whitespace(retrieved_text).lower()\n    matched = 0\n    considered = 0\n    for s in ans_sents:\n        s_norm = normalize_whitespace(s).lower()\n        if len(s_norm) < 12:\n            continue\n        considered += 1\n        if s_norm in ctx:\n            matched += 1\n    return matched / max(1, considered)\n\n\ndef retrieval_hit(answer: str, retrieved_text: str) -> bool:\n    if not answer or answer == REFUSAL_TEXT:\n        return False\n    return faithfulness_score(answer, retrieved_text) >= 0.34\n\n\ndef get_top_chunks_text(chunks: List[Dict[str, Any]], top_n: int = 3) -> str:\n    top = chunks[:top_n] if chunks else []\n    return normalize_whitespace(\" \".join([c.get(\"text_snippet\", \"\") for c in top]))\n\n\ndef run_eval(vindex: VectorIndex, bm25: BM25Index, questions: List[str]) -> List[Dict[str, Any]]:\n    rows = []\n    for q in questions:\n        out1 = ask_level1(q, vindex, top_k=TOP_K_FINAL, debug=True)\n        out2 = ask_level2(q, vindex, bm25, debug=True)\n\n        ch1 = out1.get(\"chunks\", [])\n        ch2 = out2.get(\"chunks\", [])\n\n        ctx1 = get_top_chunks_text(ch1, top_n=3)\n        ctx2 = get_top_chunks_text(ch2, top_n=3)\n\n        ans1 = out1.get(\"answer\", \"\")\n        ans2 = out2.get(\"answer\", \"\")\n\n        f1 = faithfulness_score(ans1, ctx1) if ans1 != REFUSAL_TEXT else 1.0\n        f2 = faithfulness_score(ans2, ctx2) if ans2 != REFUSAL_TEXT else 1.0\n\n        hit1 = retrieval_hit(ans1, ctx1)\n        hit2 = retrieval_hit(ans2, ctx2)\n\n        hall1 = 0.0 if ans1 == REFUSAL_TEXT else (1.0 - f1)\n        hall2 = 0.0 if ans2 == REFUSAL_TEXT else (1.0 - f2)\n\n        rows.append(\n            {\n                \"question\": q,\n                \"l1_answer\": ans1,\n                \"l2_answer\": ans2,\n                \"l1_refused\": ans1 == REFUSAL_TEXT,\n                \"l2_refused\": ans2 == REFUSAL_TEXT,\n                \"l1_hit\": bool(hit1),\n                \"l2_hit\": bool(hit2),\n                \"l1_faithfulness\": float(round(f1, 3)),\n                \"l2_faithfulness\": float(round(f2, 3)),\n                \"l1_hallucination\": float(round(hall1, 3)),\n                \"l2_hallucination\": float(round(hall2, 3)),\n                \"l1_citations\": out1.get(\"citations\", []),\n                \"l2_citations\": out2.get(\"citations\", []),\n                \"l1_top_chunks\": ch1[:3],\n                \"l2_top_chunks\": ch2[:3],\n            }\n        )\n    return rows\n\n\ndef aggregate(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n    n = len(rows)\n    if n == 0:\n        return {}\n\n    def avg(key: str) -> float:\n        return float(round(sum(r[key] for r in rows) / n, 3))\n\n    l1_ref = sum(1 for r in rows if r[\"l1_refused\"])\n    l2_ref = sum(1 for r in rows if r[\"l2_refused\"])\n\n    l1_hit = sum(1 for r in rows if r[\"l1_hit\"])\n    l2_hit = sum(1 for r in rows if r[\"l2_hit\"])\n\n    return {\n        \"n_questions\": n,\n        \"level1_refusal_rate\": round(l1_ref / n, 3),\n        \"level2_refusal_rate\": round(l2_ref / n, 3),\n        \"level1_answer_rate\": round(1 - (l1_ref / n), 3),\n        \"level2_answer_rate\": round(1 - (l2_ref / n), 3),\n        \"level1_retrieval_hit_rate\": round(l1_hit / n, 3),\n        \"level2_retrieval_hit_rate\": round(l2_hit / n, 3),\n        \"level1_faithfulness_avg\": avg(\"l1_faithfulness\"),\n        \"level2_faithfulness_avg\": avg(\"l2_faithfulness\"),\n        \"level1_hallucination_avg\": avg(\"l1_hallucination\"),\n        \"level2_hallucination_avg\": avg(\"l2_hallucination\"),\n    }\n\n\ndef pick_best_worst(rows: List[Dict[str, Any]], level: str, k: int = 5):\n    ans_key = \"l1_answer\" if level == \"l1\" else \"l2_answer\"\n    faith_key = \"l1_faithfulness\" if level == \"l1\" else \"l2_faithfulness\"\n    ref_key = \"l1_refused\" if level == \"l1\" else \"l2_refused\"\n    top_key = \"l1_top_chunks\" if level == \"l1\" else \"l2_top_chunks\"\n\n    answered = [r for r in rows if not r[ref_key]]\n    answered_sorted = sorted(answered, key=lambda r: r[faith_key], reverse=True)\n\n    best = answered_sorted[:k]\n    worst = list(reversed(answered_sorted[-k:]))\n\n    def explain(r):\n        f = r[faith_key]\n        a = r[ans_key]\n        chunks = r[top_key]\n        chunk_cites = [c.get(\"citation\") for c in chunks if c.get(\"citation\")]\n        why = \"Answer sentences are mostly present in retrieved text.\" if f >= 0.67 else \"Answer is weakly supported by retrieved text; likely missing exact sentence match.\"\n        return {\n            \"question\": r[\"question\"],\n            \"answer\": a,\n            \"faithfulness\": f,\n            \"top_chunk_citations\": chunk_cites,\n            \"explanation\": why,\n        }\n\n    return [explain(r) for r in best], [explain(r) for r in worst]\n\n\ndef write_report_full(index_dir: Path, rows: List[Dict[str, Any]], summary: Dict[str, Any]) -> Path:\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    meta_l1 = {}\n    meta_l2 = {}\n    if (index_dir / \"meta_level1.json\").exists():\n        meta_l1 = json.loads((index_dir / \"meta_level1.json\").read_text(encoding=\"utf-8\"))\n    if (index_dir / \"meta_level2.json\").exists():\n        meta_l2 = json.loads((index_dir / \"meta_level2.json\").read_text(encoding=\"utf-8\"))\n\n    best_l1, worst_l1 = pick_best_worst(rows, \"l1\", k=5)\n    best_l2, worst_l2 = pick_best_worst(rows, \"l2\", k=5)\n\n    lines = []\n    lines.append(\"# AIRMAN Evaluation Report\")\n    lines.append(\"\")\n    lines.append(\"## Level 1 metadata\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(meta_l1, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Level 2 metadata\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(meta_l2, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Baseline vs Hybrid Metrics\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(summary, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Qualitative Analysis — 5 Best (Level 1)\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(best_l1, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Qualitative Analysis — 5 Worst (Level 1)\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(worst_l1, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Qualitative Analysis — 5 Best (Level 2)\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(best_l2, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Qualitative Analysis — 5 Worst (Level 2)\")\n    lines.append(\"```json\")\n    lines.append(json.dumps(worst_l2, indent=2, ensure_ascii=False))\n    lines.append(\"```\")\n    lines.append(\"\")\n    lines.append(\"## Per-question comparison\")\n    lines.append(\"| # | Question | L1 refused | L1 hit | L1 faith | L1 halluc | L2 refused | L2 hit | L2 faith | L2 halluc |\")\n    lines.append(\"|---:|---|---:|---:|---:|---:|---:|---:|---:|---:|\")\n    for i, r in enumerate(rows, start=1):\n        q = r[\"question\"].replace(\"|\", \"\\\\|\")\n        lines.append(\n            f\"| {i} | {q} | {r['l1_refused']} | {r['l1_hit']} | {r['l1_faithfulness']} | {r['l1_hallucination']} | \"\n            f\"{r['l2_refused']} | {r['l2_hit']} | {r['l2_faithfulness']} | {r['l2_hallucination']} |\"\n        )\n\n    report_path = index_dir / \"report.md\"\n    report_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n    return report_path\n\n\nif INDEX_DIR.exists() and (INDEX_DIR / \"faiss.index\").exists():\n    vindex = load_level1(INDEX_DIR)\nelse:\n    vindex = ingest_level1(DATA_DIR, INDEX_DIR)\n\nbm25 = BM25Index.build(vindex.texts, vindex.metadatas)\nrows = run_eval(vindex, bm25, QUESTIONS)\nsummary = aggregate(rows)\nreport_path = write_report_full(INDEX_DIR, rows, summary)\nsummary, report_path"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}